{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Impact Analysis - BoVW Image Classification\n",
    "\n",
    "Comprehensive visualization of how different parameters affect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport ast\n\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context(\"talk\")\nsns.set_palette(\"husl\")\n\nTRAIN_COLOR = '#1c3b4b'\nVAL_COLOR = '#2b8183'\nTEST_COLOR = '#F18F01'\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.size'] = 11\nplt.rcParams['axes.labelsize'] = 13\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['axes.titleweight'] = 'bold'\nplt.rcParams['xtick.labelsize'] = 11\nplt.rcParams['ytick.labelsize'] = 11\nplt.rcParams['legend.fontsize'] = 11\nplt.rcParams['figure.titlesize'] = 16\nplt.rcParams['figure.titleweight'] = 'bold'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Codebook Size Impact\n",
    "\n",
    "How does vocabulary size affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_codebook = pd.read_csv('Week1/results/codebook_size_experiment_results.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_codebook.sort_values('codebook_size')\nx = df_plot['codebook_size']\n\nax.plot(x, df_plot['mean_train_acc'],\n        marker='o', linewidth=2.5, markersize=6,\n        color=TRAIN_COLOR, label='Training', alpha=0.9)\nax.fill_between(x,\n                df_plot['mean_train_acc'] - df_plot['std_train_acc'],\n                df_plot['mean_train_acc'] + df_plot['std_train_acc'],\n                color=TRAIN_COLOR, alpha=0.2)\n\nax.plot(x, df_plot['mean_val_acc'],\n        marker='s', linewidth=2.5, markersize=6,\n        color=VAL_COLOR, label='Validation', alpha=0.9)\nax.fill_between(x,\n                df_plot['mean_val_acc'] - df_plot['std_val_acc'],\n                df_plot['mean_val_acc'] + df_plot['std_val_acc'],\n                color=VAL_COLOR, alpha=0.2)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_k = df_plot.loc[best_idx, 'codebook_size']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nax.axvline(best_k, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label=f'Optimal k={int(best_k)}')\nax.scatter([best_k], [best_acc], color='red', s=150, zorder=5, marker='*')\n\nax.set_xlabel('Codebook Size (k)', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of Vocabulary Size on Performance', fontweight='bold', pad=15)\nax.legend(loc='lower right', frameon=True, shadow=True)\nax.grid(True, alpha=0.3, linestyle=':')\nax.set_xlim([x.min() - 100, x.max() + 100])\n\nplt.tight_layout()\nplt.savefig('Week1/results/codebook_size_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifier Comparison\n",
    "\n",
    "Comparing different classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_clf = pd.read_csv('Week1/results/classifier_comparison_results.csv')\n\ndef get_classifier_name(row):\n    if row['classifier'] == 'LogisticRegression':\n        return 'Logistic\\nRegression'\n    elif row['classifier'] == 'HistIntersectionSVM':\n        return 'Histogram\\nIntersection SVM'\n    else:\n        params = ast.literal_eval(row['clf_params'])\n        kernel = params.get('kernel', 'unknown')\n        return f'SVM\\n({kernel.upper()})'\n\ndf_clf['clf_name'] = df_clf.apply(get_classifier_name, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\ndf_plot = df_clf.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['clf_name'], rotation=0, ha='center')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Classifier Performance Comparison', fontweight='bold', pad=15)\nax.legend(loc='upper right', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/classifier_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM Kernel Impact\n",
    "\n",
    "How do different SVM kernels perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_kernel = pd.read_csv('Week1/results/svm_kernel_experiment_results.csv')\n\ndef extract_kernel(row):\n    if row['classifier'] == 'HistIntersectionSVM':\n        return 'Histogram\\nIntersection'\n    else:\n        params = ast.literal_eval(row['clf_params'])\n        kernel = params.get('kernel', 'unknown')\n        return kernel.upper()\n\ndf_kernel['kernel'] = df_kernel.apply(extract_kernel, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\ndf_plot = df_kernel.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['kernel'], rotation=0, ha='center')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of SVM Kernel Type', fontweight='bold', pad=15)\nax.legend(loc='upper right', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/svm_kernel_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class Weight Impact\n",
    "\n",
    "Does balancing classes improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_weight = pd.read_csv('Week1/results/svm_class_weight_experiment_results.csv')\n\ndef extract_class_weight(row):\n    params = ast.literal_eval(row['clf_params'])\n    weight = params.get('class_weight', None)\n    return 'Balanced' if weight == 'balanced' else 'None'\n\ndf_weight['class_weight'] = df_weight.apply(extract_class_weight, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(8, 6))\n\ndf_plot = df_weight.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nif len(df_plot) == 2:\n    diff = (df_plot.iloc[0]['mean_val_acc'] - df_plot.iloc[1]['mean_val_acc']) * 100\n    mid_x = 0.5\n    y_level = max(df_plot['mean_val_acc']) + max(df_plot['std_val_acc']) + 0.10\n    ax.annotate(f'{diff:+.2f}% change',\n                xy=(0, df_plot.iloc[0]['mean_val_acc']),\n                xytext=(mid_x, y_level),\n                fontsize=11, fontweight='bold', color='green' if diff > 0 else 'red',\n                ha='center')\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['class_weight'], rotation=0, ha='center')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of Class Balancing', fontweight='bold', pad=15)\nax.legend(loc='upper right', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/svm_class_weight_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary: All Parameter Impacts\n",
    "\n",
    "Combined view of all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig = plt.figure(figsize=(16, 10))\ngs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n\nax1 = fig.add_subplot(gs[0, :])\ndf_cb = df_codebook.sort_values('codebook_size')\nax1.plot(df_cb['codebook_size'], df_cb['mean_val_acc'], \n         marker='o', linewidth=2.5, markersize=7,\n         color=VAL_COLOR, label='Validation', alpha=0.9)\nax1.fill_between(df_cb['codebook_size'], \n                 df_cb['mean_val_acc'] - df_cb['std_val_acc'],\n                 df_cb['mean_val_acc'] + df_cb['std_val_acc'],\n                 color=VAL_COLOR, alpha=0.2)\nax1.set_xlabel('Codebook Size (k)', fontweight='bold')\nax1.set_ylabel('Validation Accuracy', fontweight='bold')\nax1.set_title('(A) Codebook Size Impact', fontweight='bold', fontsize=13)\nax1.grid(True, alpha=0.3, linestyle=':')\n\nax2 = fig.add_subplot(gs[1, 0])\ndf_c = df_clf.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_c))\nwidth = 0.35\nax2.bar(x_pos - width/2, df_c['mean_train_acc'], width, yerr=df_c['std_train_acc'],\n         color=TRAIN_COLOR, alpha=0.7, edgecolor='black', linewidth=1.2,\n         error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5}, label='Train')\nax2.bar(x_pos + width/2, df_c['mean_val_acc'], width, yerr=df_c['std_val_acc'],\n         color=VAL_COLOR, alpha=0.7, edgecolor='black', linewidth=1.2,\n         error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5}, label='Val')\nax2.set_xticks(x_pos)\nax2.set_xticklabels([name.replace('\\n', ' ') for name in df_c['clf_name']], fontsize=9, rotation=15, ha='right')\nax2.set_ylabel('Accuracy', fontweight='bold')\nax2.set_title('(B) Classifier Comparison', fontweight='bold', fontsize=13)\nax2.legend(loc='upper right', fontsize=9)\nax2.grid(axis='y', alpha=0.3, linestyle=':')\nax2.set_ylim([0, 1.0])\n\nax3 = fig.add_subplot(gs[1, 1])\ndf_k = df_kernel.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_k))\nax3.bar(x_pos - width/2, df_k['mean_train_acc'], width, yerr=df_k['std_train_acc'],\n         color=TRAIN_COLOR, alpha=0.7, edgecolor='black', linewidth=1.2,\n         error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5}, label='Train')\nax3.bar(x_pos + width/2, df_k['mean_val_acc'], width, yerr=df_k['std_val_acc'],\n         color=VAL_COLOR, alpha=0.7, edgecolor='black', linewidth=1.2,\n         error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5}, label='Val')\nax3.set_xticks(x_pos)\nax3.set_xticklabels([k.replace('\\n', ' ') for k in df_k['kernel']], fontsize=10)\nax3.set_ylabel('Accuracy', fontweight='bold')\nax3.set_title('(C) SVM Kernel Impact', fontweight='bold', fontsize=13)\nax3.legend(loc='upper right', fontsize=9)\nax3.grid(axis='y', alpha=0.3, linestyle=':')\nax3.set_ylim([0, 1.0])\n\nplt.savefig('Week1/results/parameter_impact_summary.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_C(row):\n    params = ast.literal_eval(row['clf_params'])\n    return params.get('C', 1.0)\n\ndf_C_svm = pd.read_csv('./Week1/results/C_parameter_svm_results.csv')\ndf_C_svm['C_value'] = df_C_svm.apply(extract_C, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_C_svm.groupby('C_value').agg({\n    'mean_train_acc': 'mean',\n    'std_train_acc': 'mean',\n    'mean_val_acc': 'mean',\n    'std_val_acc': 'mean'\n}).reset_index()\ndf_plot = df_plot.sort_values('C_value')\n\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_C = df_plot.loc[best_idx, 'C_value']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.index.get_loc(best_idx)\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nax.set_xticks(x_pos)\nax.set_xticklabels([str(c) for c in df_plot['C_value']], rotation=45, ha='right')\nax.set_xlabel('C Parameter', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of C Parameter - RBF SVM', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/C_parameter_svm_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Regularization Parameter C - RBF SVM\n",
    "\n",
    "Impact of C parameter on RBF kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_C_log = pd.read_csv('./Week1/results/C_parameter_logistic_results.csv')\ndf_C_log['C_value'] = df_C_log.apply(extract_C, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_C_log.groupby('C_value').agg({\n    'mean_train_acc': 'mean',\n    'std_train_acc': 'mean',\n    'mean_val_acc': 'mean',\n    'std_val_acc': 'mean'\n}).reset_index()\ndf_plot = df_plot.sort_values('C_value')\n\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_C = df_plot.loc[best_idx, 'C_value']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.index.get_loc(best_idx)\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nax.set_xticks(x_pos)\nax.set_xticklabels([str(c) for c in df_plot['C_value']], rotation=45, ha='right')\nax.set_xlabel('C Parameter', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of C Parameter - Logistic Regression', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/C_parameter_logistic_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Regularization Parameter C - Logistic Regression\n",
    "\n",
    "Impact of C parameter on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_C_hist = pd.read_csv('./Week1/results/C_parameter_histogram_results.csv')\ndf_C_hist['C_value'] = df_C_hist.apply(extract_C, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_C_hist.groupby('C_value').agg({\n    'mean_train_acc': 'mean',\n    'std_train_acc': 'mean',\n    'mean_val_acc': 'mean',\n    'std_val_acc': 'mean'\n}).reset_index()\ndf_plot = df_plot.sort_values('C_value')\n\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_C = df_plot.loc[best_idx, 'C_value']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.index.get_loc(best_idx)\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nax.set_xticks(x_pos)\nax.set_xticklabels([str(c) for c in df_plot['C_value']], rotation=45, ha='right')\nax.set_xlabel('C Parameter', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of C Parameter - Histogram Intersection SVM', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/C_parameter_histogram_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Regularization Parameter C - Histogram Intersection SVM\n",
    "\n",
    "Impact of C parameter on Histogram Intersection SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_nfeatures = pd.read_csv('Week1/results/sift_nfeatures_test_results.csv')\n\ndef extract_nfeatures(row):\n    detector = row['detector']\n    if 'nf' in detector:\n        import re\n        match = re.search(r'nf(\\d+)', detector)\n        if match:\n            return int(match.group(1))\n    return None\n\ndf_nfeatures['nfeatures'] = df_nfeatures.apply(extract_nfeatures, axis=1)\ndf_nfeatures = df_nfeatures.dropna(subset=['nfeatures'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_nfeatures.groupby('nfeatures').agg({\n    'mean_train_acc': 'mean',\n    'std_train_acc': 'mean',\n    'mean_val_acc': 'mean',\n    'std_val_acc': 'mean'\n}).reset_index()\ndf_plot = df_plot.sort_values('nfeatures')\n\nx = df_plot['nfeatures']\n\nax.plot(x, df_plot['mean_train_acc'],\n        marker='o', linewidth=2.5, markersize=6,\n        color=TRAIN_COLOR, label='Training', alpha=0.9)\nax.fill_between(x,\n                df_plot['mean_train_acc'] - df_plot['std_train_acc'],\n                df_plot['mean_train_acc'] + df_plot['std_train_acc'],\n                color=TRAIN_COLOR, alpha=0.2)\n\nax.plot(x, df_plot['mean_val_acc'],\n        marker='s', linewidth=2.5, markersize=6,\n        color=VAL_COLOR, label='Validation', alpha=0.9)\nax.fill_between(x,\n                df_plot['mean_val_acc'] - df_plot['std_val_acc'],\n                df_plot['mean_val_acc'] + df_plot['std_val_acc'],\n                color=VAL_COLOR, alpha=0.2)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_nf = df_plot.loc[best_idx, 'nfeatures']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nax.axvline(best_nf, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label=f'Optimal nf={int(best_nf)}')\nax.scatter([best_nf], [best_acc], color='red', s=150, zorder=5, marker='*')\n\nax.set_xlabel('Number of SIFT Features', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of SIFT nFeatures Parameter', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(True, alpha=0.3, linestyle=':')\n\nplt.tight_layout()\nplt.savefig('Week1/results/sift_nfeatures_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SIFT nFeatures Impact\n",
    "\n",
    "How does the number of SIFT features affect performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_gamma = pd.read_csv('./Week1/results/svm_gamma_experiment_results.csv')\n\ndef extract_gamma(row):\n    params = ast.literal_eval(row['clf_params'])\n    gamma = params.get('gamma', 'scale')\n    return gamma\n\ndf_gamma['gamma_value'] = df_gamma.apply(extract_gamma, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(14, 6))\n\ndef gamma_sort_key(gamma_val):\n    if isinstance(gamma_val, (int, float)):\n        return (0, gamma_val)\n    else:\n        order = {'auto': 1, 'scale': 2}\n        return (1, order.get(gamma_val, 999))\n\ndf_plot = df_gamma.copy()\ndf_plot['sort_key'] = df_plot['gamma_value'].apply(gamma_sort_key)\ndf_plot = df_plot.sort_values('sort_key')\n\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=7)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=7)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_gamma = df_plot.loc[best_idx, 'gamma_value']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.reset_index(drop=True)[df_plot.index == best_idx].index[0]\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nx_labels = []\nfor val in df_plot['gamma_value']:\n    if isinstance(val, (int, float)):\n        x_labels.append(f'{val:.4f}' if val < 0.01 else f'{val}')\n    else:\n        x_labels.append(val)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(x_labels, rotation=45, ha='right')\nax.set_xlabel('Gamma Parameter', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of SVM Gamma Parameter', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/svm_gamma_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SVM Gamma Parameter Impact\n",
    "\n",
    "How does the gamma parameter affect SVM RBF kernel performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_pyramid = pd.read_csv('./Week1/results/spatial_pyramid_experiment_results.csv')\n\ndef get_pyramid_name(row):\n    pyramid_type = str(row['spatial_pyramid'])\n    if pyramid_type == 'None':\n        return 'None'\n    elif pyramid_type == 'horizontal':\n        return 'Horizontal'\n    elif pyramid_type == 'vertical':\n        return 'Vertical'\n    elif pyramid_type == 'square':\n        return 'Square'\n    else:\n        return pyramid_type\n\ndf_pyramid['pyramid_name'] = df_pyramid.apply(get_pyramid_name, axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pyramid_types = ['horizontal', 'vertical', 'square']\npyramid_types_present = [pt for pt in pyramid_types if pt.capitalize() in df_pyramid['pyramid_name'].values]\n\nif len(pyramid_types_present) > 0:\n    fig, axes = plt.subplots(1, len(pyramid_types_present), figsize=(6*len(pyramid_types_present), 5))\n    if len(pyramid_types_present) == 1:\n        axes = [axes]\n    \n    for idx, ptype in enumerate(pyramid_types_present):\n        ax = axes[idx]\n        \n        df_plot = df_pyramid[df_pyramid['pyramid_name'] == ptype.capitalize()].copy()\n        \n        df_none = df_pyramid[df_pyramid['pyramid_name'] == 'None'].copy()\n        if len(df_none) > 0:\n            df_none['pyramid_levels'] = 0\n            df_plot = pd.concat([df_none, df_plot])\n        \n        df_plot = df_plot.sort_values('pyramid_levels')\n        \n        x = df_plot['pyramid_levels']\n        \n        ax.plot(x, df_plot['mean_train_acc'],\n                marker='o', linewidth=2.5, markersize=6,\n                color=TRAIN_COLOR, label='Training', alpha=0.9)\n        ax.fill_between(x,\n                        df_plot['mean_train_acc'] - df_plot['std_train_acc'],\n                        df_plot['mean_train_acc'] + df_plot['std_train_acc'],\n                        color=TRAIN_COLOR, alpha=0.2)\n        \n        ax.plot(x, df_plot['mean_val_acc'],\n                marker='s', linewidth=2.5, markersize=6,\n                color=VAL_COLOR, label='Validation', alpha=0.9)\n        ax.fill_between(x,\n                        df_plot['mean_val_acc'] - df_plot['std_val_acc'],\n                        df_plot['mean_val_acc'] + df_plot['std_val_acc'],\n                        color=VAL_COLOR, alpha=0.2)\n        \n        df_plot_no_none = df_plot[df_plot['pyramid_levels'] > 0]\n        if len(df_plot_no_none) > 0:\n            best_idx = df_plot_no_none['mean_val_acc'].idxmax()\n            best_levels = df_plot_no_none.loc[best_idx, 'pyramid_levels']\n            best_acc = df_plot_no_none.loc[best_idx, 'mean_val_acc']\n            ax.axvline(best_levels, color='red', linestyle='--', linewidth=1.5, alpha=0.5, \n                      label=f'Optimal levels={int(best_levels)}')\n            ax.scatter([best_levels], [best_acc], color='red', s=150, zorder=5, marker='*')\n        \n        ax.set_xlabel('Pyramid Levels (0=None)', fontweight='bold')\n        ax.set_ylabel('Accuracy', fontweight='bold')\n        ax.set_title(f'{ptype.capitalize()} Pyramid', fontweight='bold', pad=15)\n        ax.legend(loc='best', frameon=True, shadow=True, fontsize=9)\n        ax.grid(True, alpha=0.3, linestyle=':')\n        ax.set_ylim([0, 1.0])\n    \n    plt.tight_layout()\n    plt.savefig('Week1/results/spatial_pyramid_impact.png', dpi=300, bbox_inches='tight')\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spatial Pyramid Impact\n",
    "\n",
    "How does spatial pyramid pooling affect performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_pca = pd.read_csv('./Week1/results/pca_dimensionality_results.csv')\n\ndf_pca['pca_dimension'] = df_pca['pca_dim'].fillna(0).astype(int)\ndf_pca = df_pca[df_pca['pca_dimension'] > 0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_pca.groupby('pca_dimension').agg({\n    'mean_train_acc': 'mean',\n    'std_train_acc': 'mean',\n    'mean_val_acc': 'mean',\n    'std_val_acc': 'mean',\n    'pca_explained_variance': 'mean'\n}).reset_index()\ndf_plot = df_plot.sort_values('pca_dimension')\n\nx = df_plot['pca_dimension']\n\nax.plot(x, df_plot['mean_train_acc'],\n        marker='o', linewidth=2.5, markersize=6,\n        color=TRAIN_COLOR, label='Training', alpha=0.9)\nax.fill_between(x,\n                df_plot['mean_train_acc'] - df_plot['std_train_acc'],\n                df_plot['mean_train_acc'] + df_plot['std_train_acc'],\n                color=TRAIN_COLOR, alpha=0.2)\n\nax.plot(x, df_plot['mean_val_acc'],\n        marker='s', linewidth=2.5, markersize=6,\n        color=VAL_COLOR, label='Validation', alpha=0.9)\nax.fill_between(x,\n                df_plot['mean_val_acc'] - df_plot['std_val_acc'],\n                df_plot['mean_val_acc'] + df_plot['std_val_acc'],\n                color=VAL_COLOR, alpha=0.2)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_dim = df_plot.loc[best_idx, 'pca_dimension']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nax.axvline(best_dim, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label=f'Optimal dim={int(best_dim)}')\nax.scatter([best_dim], [best_acc], color='red', s=150, zorder=5, marker='*')\n\nax.set_xlabel('PCA Dimensions', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of PCA Dimensionality Reduction', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(True, alpha=0.3, linestyle=':')\n\nplt.tight_layout()\nplt.savefig('Week1/results/pca_dimensionality_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Dense SIFT Scale Impact\n\nHow does the scale parameter affect Dense SIFT performance?"
  },
  {
   "cell_type": "code",
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\ndf_plot = df_detector.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['detector'], rotation=0, ha='center')\nax.set_xlabel('Feature Detector', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Feature Detector Comparison', fontweight='bold', pad=15)\nax.legend(loc='upper right', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/detector_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_detector = pd.read_csv('./Week1/results/detector_comparison_test_results.csv')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Detector Comparison\n\nComparing different feature detectors: SIFT, ORB, and AKAZE",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\ndf_plot = df_dense_step.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_step = df_plot.loc[best_idx, 'dense_step']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.reset_index(drop=True)[df_plot.index == best_idx].index[0]\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['dense_step'], rotation=0, ha='center')\nax.set_xlabel('Dense SIFT Step Size', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of Dense SIFT Step Parameter', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/dense_sift_step_impact.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_dense_step = pd.read_csv('./Week1/results/dense_sift_step_test_results.csv')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dense SIFT Step Impact\n\nHow does the step parameter affect Dense SIFT performance?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig, ax = plt.subplots(figsize=(12, 6))\n\ndf_plot = df_dense_scale.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=8)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_scale = df_plot.loc[best_idx, 'scales_label']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.reset_index(drop=True)[df_plot.index == best_idx].index[0]\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['scales_label'], rotation=45, ha='right', fontsize=9)\nax.set_xlabel('Dense SIFT Scales', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of Dense SIFT Scale Parameter', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/dense_sift_scale_impact.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_dense_scale = pd.read_csv('./Week1/results/dense_sift_scale_test_results.csv')\n\ndef extract_dense_scales(row):\n    scales_str = str(row['dense_scales'])\n    scales = ast.literal_eval(scales_str)\n    if isinstance(scales, list):\n        return str(scales)\n    return str([scales])\n\ndf_dense_scale['scales_label'] = df_dense_scale.apply(extract_dense_scales, axis=1)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_detector = pd.read_csv('./Week1/results/detector_comparison_test_results.csv')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\ndf_plot = df_dense_step.sort_values('mean_val_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nbest_idx = df_plot['mean_val_acc'].idxmax()\nbest_step = df_plot.loc[best_idx, 'dense_step']\nbest_acc = df_plot.loc[best_idx, 'mean_val_acc']\nbest_x_pos = df_plot.reset_index(drop=True)[df_plot.index == best_idx].index[0]\nax.scatter([best_x_pos], [best_acc], color='red', s=200, zorder=5, marker='*')\n\nax.set_xticks(x_pos)\nax.set_xticklabels(df_plot['dense_step'], rotation=0, ha='center')\nax.set_xlabel('Dense SIFT Step Size', fontweight='bold')\nax.set_ylabel('Accuracy', fontweight='bold')\nax.set_title('Impact of Dense SIFT Step Parameter', fontweight='bold', pad=15)\nax.legend(loc='best', frameon=True, shadow=True)\nax.grid(axis='y', alpha=0.3, linestyle=':')\nax.set_ylim([0, 1.0])\n\nplt.tight_layout()\nplt.savefig('Week1/results/dense_sift_step_impact.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dense SIFT Step Impact\n\nHow does the step parameter affect Dense SIFT performance?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "df_dense_scale = pd.read_csv('./Week1/results/dense_sift_scale_test_results.csv')\n\ndef extract_dense_scales(row):\n    scales_str = str(row['dense_scales'])\n    scales = ast.literal_eval(scales_str)\n    if isinstance(scales, list):\n        return str(scales)\n    return str([scales])\n\ndf_dense_scale['scales_label'] = df_dense_scale.apply(extract_dense_scales, axis=1)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_scaler = pd.read_csv('Week1/results/encoding_scaler_test_results.csv')\n\ndf_scaler['scaler_type'] = df_scaler['scaler_type'].fillna('none')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\ndf_plot = df_scaler.sort_values('test_acc', ascending=False)\nx_pos = np.arange(len(df_plot))\nwidth = 0.35\n\nbars1 = ax1.bar(x_pos - width/2, df_plot['mean_train_acc'],\n       width, yerr=df_plot['std_train_acc'],\n       color=TRAIN_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Training (±std)')\n\nbars2 = ax1.bar(x_pos + width/2, df_plot['mean_val_acc'],\n       width, yerr=df_plot['std_val_acc'],\n       color=VAL_COLOR, edgecolor='black', linewidth=1.5,\n       error_kw={'linewidth': 2, 'ecolor': 'black', 'capsize': 5},\n       label='Validation (±std)')\n\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n            f'{height:.3f}',\n            ha='center', va='bottom', fontsize=9)\n\nax1.set_xticks(x_pos)\nax1.set_xticklabels(df_plot['scaler_type'], rotation=0, ha='center')\nax1.set_ylabel('Accuracy', fontweight='bold')\nax1.set_title('Train vs Validation Accuracy by Scaler', fontweight='bold', pad=15)\nax1.legend(loc='upper right', frameon=True, shadow=True)\nax1.grid(axis='y', alpha=0.3, linestyle=':')\nax1.set_ylim([0, 1.0])\n\ndf_plot['train_val_gap'] = df_plot['mean_train_acc'] - df_plot['mean_val_acc']\n\nbars = ax2.bar(x_pos, df_plot['train_val_gap'], \n               color=[TEST_COLOR if x > 0.4 else TRAIN_COLOR for x in df_plot['train_val_gap']],\n               edgecolor='black', linewidth=1.5)\n\nfor i, (bar, val) in enumerate(zip(bars, df_plot['train_val_gap'])):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f'{val:.3f}',\n            ha='center', va='bottom' if height > 0 else 'top', fontsize=10, fontweight='bold')\n\nax2.set_xticks(x_pos)\nax2.set_xticklabels(df_plot['scaler_type'], rotation=0, ha='center')\nax2.set_ylabel('Train - Validation Gap', fontweight='bold')\nax2.set_title('Overfitting: Train-Validation Gap', fontweight='bold', pad=15)\nax2.grid(axis='y', alpha=0.3, linestyle=':')\nax2.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n\nplt.tight_layout()\nplt.savefig('Week1/results/encoding_scaler_impact.png', dpi=300, bbox_inches='tight')\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}